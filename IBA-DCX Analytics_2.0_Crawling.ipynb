{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in /usr/local/lib/python3.6/dist-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in ./.local/lib/python3.6/site-packages (from selenium) (1.26.15)\n",
      "패키지 목록을 읽는 중입니다... 완료0%\n",
      "E: 잠금 파일 /var/lib/apt/lists/lock 파일을 열 수 없습니다 - open (13: Permission denied)\n",
      "E: /var/lib/apt/lists/ 디렉터리를 잠글 수 없습니다\n",
      "W: /var/cache/apt/pkgcache.bin 파일을 삭제하는데 문제가 있습니다 - RemoveCaches (13: Permission denied)\n",
      "W: /var/cache/apt/srcpkgcache.bin 파일을 삭제하는데 문제가 있습니다 - RemoveCaches (13: Permission denied)\n",
      "E: 잠금 파일 /var/lib/dpkg/lock-frontend 파일을 열 수 없습니다 - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "cp: cannot stat '/usr/lib/chromium-browser/chromedriver': No such file or directory\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fake_useragent in ./.local/lib/python3.6/site-packages (1.1.3)\n",
      "Requirement already satisfied: importlib-resources>=5.0 in ./.local/lib/python3.6/site-packages (from fake_useragent) (5.4.0)\n",
      "Requirement already satisfied: importlib-metadata~=4.0 in ./.local/lib/python3.6/site-packages (from fake_useragent) (4.8.3)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.local/lib/python3.6/site-packages (from importlib-metadata~=4.0->fake_useragent) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in ./.local/lib/python3.6/site-packages (from importlib-metadata~=4.0->fake_useragent) (4.1.1)\n",
      "E: 잠금 파일 /var/lib/dpkg/lock-frontend 파일을 열 수 없습니다 - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: JPype1-py3 in ./.local/lib/python3.6/site-packages (0.5.5.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!apt-get update\n",
    "!apt install chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "!apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
    "!pip3 install JPype1-py3\n",
    "!pip install beautifulsoup4\n",
    "!pip install sklearn\n",
    "!pip install fake-useragent\n",
    "!pip install pygame\n",
    "!pip install --upgrade fake-useragent\n",
    "!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/lib/python3/dist-packages (4.6.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in ./.local/lib/python3.6/site-packages (0.0.post1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fake-useragent in ./.local/lib/python3.6/site-packages (1.1.3)\n",
      "Requirement already satisfied: importlib-resources>=5.0 in ./.local/lib/python3.6/site-packages (from fake-useragent) (5.4.0)\n",
      "Requirement already satisfied: importlib-metadata~=4.0 in ./.local/lib/python3.6/site-packages (from fake-useragent) (4.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in ./.local/lib/python3.6/site-packages (from importlib-metadata~=4.0->fake-useragent) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.local/lib/python3.6/site-packages (from importlib-metadata~=4.0->fake-useragent) (3.6.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pygame in ./.local/lib/python3.6/site-packages (2.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fake-useragent in ./.local/lib/python3.6/site-packages (1.1.3)\n",
      "Requirement already satisfied: importlib-resources>=5.0 in ./.local/lib/python3.6/site-packages (from fake-useragent) (5.4.0)\n",
      "Requirement already satisfied: importlib-metadata~=4.0 in ./.local/lib/python3.6/site-packages (from fake-useragent) (4.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in ./.local/lib/python3.6/site-packages (from importlib-metadata~=4.0->fake-useragent) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.local/lib/python3.6/site-packages (from importlib-metadata~=4.0->fake-useragent) (3.6.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: webdriver-manager in ./.local/lib/python3.6/site-packages (3.7.1)\n",
      "Requirement already satisfied: python-dotenv in ./.local/lib/python3.6/site-packages (from webdriver-manager) (0.20.0)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.6/site-packages (from webdriver-manager) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->webdriver-manager) (2018.1.18)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.local/lib/python3.6/site-packages (from requests->webdriver-manager) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.local/lib/python3.6/site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./.local/lib/python3.6/site-packages (from requests->webdriver-manager) (2.0.12)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import ElementClickInterceptedException, NoSuchElementException, StaleElementReferenceException, ElementNotInteractableException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from PIL import Image\n",
    "from pygame import mixer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fake_useragent import UserAgent\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tourist destination</th>\n",
       "      <th>dong</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>차이나타운</td>\n",
       "      <td>초량 1동</td>\n",
       "      <td>마루팥빙수단팥죽 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>차이나타운</td>\n",
       "      <td>초량 1동</td>\n",
       "      <td>고민끝에여기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>차이나타운</td>\n",
       "      <td>초량 1동</td>\n",
       "      <td>홍성방 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>차이나타운</td>\n",
       "      <td>초량 1동</td>\n",
       "      <td>해신자연산횟집</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>차이나타운</td>\n",
       "      <td>초량 1동</td>\n",
       "      <td>시먼띵</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>부산아시아드조각광장</td>\n",
       "      <td>사직동</td>\n",
       "      <td>쭈꾸미본색 연산점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>부산아시아드조각광장</td>\n",
       "      <td>사직동</td>\n",
       "      <td>아쿠마닭갈비포차</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>부산아시아드조각광장</td>\n",
       "      <td>사직동</td>\n",
       "      <td>부잣집 동태하우스 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>부산아시아드조각광장</td>\n",
       "      <td>사직동</td>\n",
       "      <td>청솔물회</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>부산아시아드조각광장</td>\n",
       "      <td>사직동</td>\n",
       "      <td>용문각</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Tourist destination   dong          name\n",
       "0                 차이나타운  초량 1동   마루팥빙수단팥죽 본점\n",
       "1                 차이나타운  초량 1동        고민끝에여기\n",
       "2                 차이나타운  초량 1동        홍성방 본점\n",
       "3                 차이나타운  초량 1동       해신자연산횟집\n",
       "4                 차이나타운  초량 1동           시먼띵\n",
       "..                  ...    ...           ...\n",
       "115          부산아시아드조각광장    사직동     쭈꾸미본색 연산점\n",
       "116          부산아시아드조각광장    사직동      아쿠마닭갈비포차\n",
       "117          부산아시아드조각광장    사직동  부잣집 동태하우스 본점\n",
       "118          부산아시아드조각광장    사직동          청솔물회\n",
       "119          부산아시아드조각광장    사직동           용문각\n",
       "\n",
       "[120 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# csv 파일이되 데이터 구분을 '|' 로 해둔 파일입니다. sep 지정을 안 하면 읽을 수 없습니다.\n",
    "df = pd.read_csv('소상공인시장진흥공단_상가(상권)정보_제주_202212.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 음식점 데이터만 쓸 겁니다\n",
    "df = df.loc[df['상권업종대분류명'] == '음식']  \n",
    "\n",
    "\n",
    "# 다음과 같은 칼럼만 있으면 됩니다\n",
    "df = df[['상호명', '상권업종중분류명', '상권업종소분류명', '표준산업분류명', '행정동명', '위도', '경도']]\n",
    "\n",
    "# 칼럼명 단순화\n",
    "\n",
    "df.columns = ['name',  # 상호명\n",
    "              'cate_1',  # 중분류명\n",
    "              'cate_2',  # 소분류명\n",
    "              'cate_3',  # 표준산업분류명\n",
    "              'dong',  # 행정동명\n",
    "              'lon',  # 위도\n",
    "              'lat'  # 경도\n",
    "              ]\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # 피체 벡터화\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # 코사인 유사도\n",
    "\n",
    "\n",
    "count_vect_category = CountVectorizer(min_df=0, ngram_range=(1,2))\n",
    "place_category = count_vect_category.fit_transform(df['cate_mix'].values.astype( 'U' ))\n",
    "place_simi_cate = cosine_similarity(place_category, place_category) \n",
    "place_simi_cate_sorted_ind = place_simi_cate.argsort()[:, ::-1]\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "#ua = UserAgent()\n",
    "#user_agent = ua.random\n",
    "#options.add_argument(f'user-agent={user_agent}')\n",
    "options.add_argument('--headless') # Set Head-less\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "#driver = webdriver.Chrome('chromedriver', options=options)\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "\n",
    "\n",
    "df['naver_keyword'] = df['dong'] + \"%20\" + df['name']  # \"%20\"는 띄어쓰기를 의미합니다.\n",
    "df['naver_map_url'] = ''\n",
    "\n",
    "\n",
    "# 본격적으로 가게 상세페이지의 URL을 가져옵시다\n",
    "\n",
    "for i, keyword in enumerate(df['naver_keyword'].tolist()):\n",
    "    print(\"이번에 찾을 키워드 :\", i, f\"/ {df.shape[0] -1} 행\", keyword)\n",
    "    try:\n",
    "        naver_map_search_url = f\"https://m.map.naver.com/search2/search.naver?query={keyword}&sm=hty&style=v5\"\n",
    "        \n",
    "        driver.get(naver_map_search_url)\n",
    "        time.sleep(3)\n",
    "        df.iloc[i,-1] = driver.find_element(By.CSS_SELECTOR, \"#ct > div.search_listview._content._ctList > ul > li:nth-child(1) > div.item_info > a.a_item.a_item_distance._linkSiteview\").get_attribute('data-cid')\n",
    "        # 네이버 지도 시스템은 data-cid에 url 파라미터를 저장해두고 있었습니다.\n",
    "        # data-cid 번호를 뽑아두었다가 기본 url 템플릿에 넣어 최종적인 url을 완성하면 됩니다.\n",
    "        \n",
    "        #만약 검색 결과가 없다면?\n",
    "    except Exception as e1:\n",
    "        if \"li:nth-child(1)\" in str(e1):  # -> \"child(1)이 없던데요?\"\n",
    "            try:\n",
    "                df.iloc[i,-1] = driver.find_element(By.CSS_SELECTOR, \"#ct > div.search_listview._content._ctList > ul > li:nth-child(1) > div.item_info > a.a_item.a_item_distance._linkSiteview\").get_attribute('data-cid')\n",
    "                time.sleep(1)\n",
    "            except Exception as e2:\n",
    "                print(e2)\n",
    "                df.iloc[i,-1] = np.nan\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# URL이 수집되지 않은 데이터는 제거합니다.\n",
    "df = df.loc[~df['naver_map_url'].isnull()]\n",
    "\n",
    "#df['naver_map_url']은 각 가게별 Key값으로 사용 가능\n",
    "\n",
    "# 이때 수집한 것은 완전한 URL이 아니라 URL에 들어갈 ID (data-cid 라는 코드명으로 저장된) 이므로, 온전한 URL로 만들어줍니다\n",
    "df['가게정보'] = \"https://pcmap.place.naver.com/restaurant/\" + df['naver_map_url']+\"/home\"\n",
    "df['리뷰링크'] = \"https://pcmap.place.naver.com/restaurant/\" + df['naver_map_url']+\"/review/visitor\"\n",
    "\n",
    "df.to_csv('제주도_음식점_주소.csv', encoding = 'utf-8-sig', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가게별 '리뷰' 크롤링\n",
    "# 파일 로드\n",
    "result1 = pd.read_csv('제주도_음식점_주소.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 웹드라이버 옵션 설정\n",
    "options = webdriver.ChromeOptions()\n",
    "#ua = UserAgent()\n",
    "#user_agent = ua.random\n",
    "#options.add_argument(f'user-agent={user_agent}')\n",
    "options.add_argument('--headless') # Set Head-less\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "#driver = webdriver.Chrome('chromedriver', options=options)\n",
    "\n",
    "\n",
    "# 웹드라이버 초기화\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "\n",
    "restaurant_name = []\n",
    "restaurant_url = []\n",
    "tourist_destination = []  \n",
    "user_id = []\n",
    "content = []\n",
    "keywords = []\n",
    "image_links = []  \n",
    "author_names = []  # Add this line for author names\n",
    "sub_counter = 0\n",
    "counter = 0\n",
    "sub_sub_counter = 0\n",
    "\n",
    "# URL에 접속하여 크롤링 수행\n",
    "for idx in range(len(result1['리뷰링크'])):\n",
    "    counter = 0\n",
    "    sub_counter = 0\n",
    "    sub_sub_counter += 1\n",
    "    driver.get(result1['리뷰링크'][idx])\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)\n",
    "            driver.implicitly_wait(1)\n",
    "            button = driver.find_element(By.CSS_SELECTOR, 'div.lfH3O a.fvwqf')\n",
    "            button.click()\n",
    "            counter += 1\n",
    "            print(sub_sub_counter, \"번째 가게의 더보기\", counter, \"번째 누르는 중\")\n",
    "            driver.implicitly_wait(1)\n",
    "            driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)\n",
    "            driver.implicitly_wait(1)\n",
    "            if counter == 100:\n",
    "                print(sub_sub_counter, \"번째 가게는 1000개 초과-\")\n",
    "                break\n",
    "        except NoSuchElementException:\n",
    "            print(sub_sub_counter, \"번째 가게 리뷰 더보기 완료-\")\n",
    "            break\n",
    "        except ElementClickInterceptedException:\n",
    "            print(sub_sub_counter, \"번째 가게는 막혀서 패스-\")\n",
    "            break\n",
    "        except StaleElementReferenceException:\n",
    "            print(\"StaleElementReferenceException가 발생했습니다. 요소를 다시 찾는 중입니다...\")\n",
    "\n",
    "    buttons = driver.find_elements(By.CSS_SELECTOR, \".P1zUJ.ZGKcF\")\n",
    "    for button in buttons:\n",
    "        try:\n",
    "            button.click()\n",
    "            driver.implicitly_wait(1)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    contents = soup.find_all('li', 'YeINN')\n",
    "\n",
    "    for i in range(len(contents)):\n",
    "        restaurant_name.append(result1['name'][idx])\n",
    "        restaurant_url.append(result1['리뷰링크'][idx])\n",
    "        tourist_destination.append(result1['Tourist destination'][idx])  # Append tourist destination\n",
    "\n",
    "        # Extract author name\n",
    "        author_name_div = contents[i].find('div', {'class': 'VYGLG'})\n",
    "        if author_name_div:\n",
    "            author_names.append(author_name_div.text)\n",
    "        else:\n",
    "            author_names.append(None)\n",
    "        user_div = contents[i].find('div', attrs={'class': 'p24Ki'})\n",
    "        if user_div:\n",
    "            user_id.append(user_div.text)\n",
    "        else:\n",
    "            user_id.append(None)\n",
    "\n",
    "        try:\n",
    "            content_text = contents[i].find('span', attrs={'class': 'zPfVt'}).text.replace('\\n', ' ')\n",
    "        except AttributeError:\n",
    "            content_text = None\n",
    "        content.append(content_text)\n",
    "\n",
    "        gyagi_div = contents[i].find('div', attrs={'class': 'gyAGI'})\n",
    "        if gyagi_div:\n",
    "            keyword_elements = gyagi_div.find_all('span', attrs={'class': 'P1zUJ'})\n",
    "            keyword_texts = [keyword_element.text for keyword_element in keyword_elements]\n",
    "            keywords.append(keyword_texts if keyword_texts else None)\n",
    "        else:\n",
    "            keywords.append(None)\n",
    "\n",
    "        # Extracting image links\n",
    "        image_div = contents[i].find('div', {'class': 'ODalI'})\n",
    "        if image_div:\n",
    "            style_attr = image_div.find('div', {'class': 'K0PDV'})['style']\n",
    "            image_url = re.search(r'url\\(\"(.+?)\"\\)', style_attr)\n",
    "            if image_url:\n",
    "                image_links.append(image_url.group(1))\n",
    "            else:\n",
    "                image_links.append(None)\n",
    "        else:\n",
    "            image_links.append(None)\n",
    "    sub_counter += 1\n",
    "\n",
    "    print(sub_sub_counter, \"번째 가게까지\", len(user_id), \"개 리뷰 수집 완료.\")\n",
    "\n",
    "# 데이터프레임 생성\n",
    "naver_review = pd.DataFrame({\n",
    "    'Name': restaurant_name,\n",
    "    'URL': restaurant_url,\n",
    "    'Tourist_destination': tourist_destination,\n",
    "    'Author_Name': author_names,  # Add this line for author names\n",
    "    'ID': user_id,\n",
    "    'Content': content,\n",
    "    'Keywords': keywords,\n",
    "    'Image Links': image_links\n",
    "})\n",
    "\n",
    "# CSV 파일로 저장\n",
    "naver_review.to_csv('제주도_음식점_리뷰_크롤링_완료.csv', encoding='utf-8-sig', index=True)\n",
    "\n",
    "# 데이터프레임 출력\n",
    "naver_review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 87.0.4280\n",
      "[WDM] - Get LATEST chromedriver version for 87.0.4280 google-chrome\n",
      "[WDM] - Driver [/home/nampromotion/.wdm/drivers/chromedriver/linux64/87.0.4280.88/chromedriver] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling completed for store 1\n",
      "Crawling completed for store 2\n",
      "Crawling completed for store 3\n",
      "Crawling completed for store 4\n",
      "Crawling completed for store 5\n",
      "Crawling completed for store 6\n",
      "Crawling completed for store 7\n",
      "Crawling completed for store 8\n",
      "Crawling completed for store 9\n",
      "Crawling completed for store 10\n",
      "Crawling completed for store 11\n",
      "Crawling completed for store 12\n",
      "Crawling completed for store 13\n",
      "Crawling completed for store 14\n",
      "Crawling completed for store 15\n",
      "Crawling completed for store 16\n",
      "Crawling completed for store 17\n",
      "Crawling completed for store 18\n",
      "Crawling completed for store 19\n",
      "Crawling completed for store 20\n",
      "Crawling completed for store 21\n",
      "Crawling completed for store 22\n",
      "Crawling completed for store 23\n",
      "Crawling completed for store 24\n",
      "Crawling completed for store 25\n",
      "Crawling completed for store 26\n",
      "Crawling completed for store 27\n",
      "Crawling completed for store 28\n",
      "Crawling completed for store 29\n",
      "Crawling completed for store 30\n",
      "Crawling completed for store 31\n",
      "Crawling completed for store 32\n",
      "Crawling completed for store 33\n",
      "Crawling completed for store 34\n",
      "Crawling completed for store 35\n",
      "Crawling completed for store 36\n",
      "Crawling completed for store 37\n",
      "Crawling completed for store 38\n",
      "Crawling completed for store 39\n",
      "Crawling completed for store 40\n",
      "Crawling completed for store 41\n",
      "Crawling completed for store 42\n",
      "Crawling completed for store 43\n",
      "Crawling completed for store 44\n",
      "Crawling completed for store 45\n",
      "Crawling completed for store 46\n",
      "Crawling completed for store 47\n",
      "Crawling completed for store 48\n",
      "Crawling completed for store 49\n",
      "Crawling completed for store 50\n",
      "Crawling completed for store 51\n",
      "Crawling completed for store 52\n",
      "Crawling completed for store 53\n",
      "Crawling completed for store 54\n",
      "Crawling completed for store 55\n",
      "Crawling completed for store 56\n",
      "Crawling completed for store 57\n",
      "Crawling completed for store 58\n",
      "Crawling completed for store 59\n",
      "Crawling completed for store 60\n",
      "Crawling completed for store 61\n",
      "Crawling completed for store 62\n",
      "Crawling completed for store 63\n",
      "Crawling completed for store 64\n",
      "Crawling completed for store 65\n",
      "Crawling completed for store 66\n",
      "Crawling completed for store 67\n",
      "Crawling completed for store 68\n",
      "Crawling completed for store 69\n",
      "Crawling completed for store 70\n",
      "Crawling completed for store 71\n",
      "Crawling completed for store 72\n",
      "Crawling completed for store 73\n",
      "Crawling completed for store 74\n",
      "Crawling completed for store 75\n",
      "Crawling completed for store 76\n",
      "Crawling completed for store 77\n",
      "Crawling completed for store 78\n",
      "Crawling completed for store 79\n",
      "Crawling completed for store 80\n",
      "Crawling completed for store 81\n",
      "Crawling completed for store 82\n",
      "Crawling completed for store 83\n",
      "Crawling completed for store 84\n",
      "Crawling completed for store 85\n",
      "Crawling completed for store 86\n",
      "Crawling completed for store 87\n",
      "Crawling completed for store 88\n",
      "Crawling completed for store 89\n",
      "Crawling completed for store 90\n",
      "Crawling completed for store 91\n",
      "Crawling completed for store 92\n",
      "Crawling completed for store 93\n",
      "Crawling completed for store 94\n",
      "Crawling completed for store 95\n",
      "Crawling completed for store 96\n",
      "Crawling completed for store 97\n",
      "Crawling completed for store 98\n",
      "Crawling completed for store 99\n",
      "Crawling completed for store 100\n",
      "Crawling completed for store 101\n",
      "Crawling completed for store 102\n",
      "Crawling completed for store 103\n",
      "Crawling completed for store 104\n",
      "Crawling completed for store 105\n",
      "Crawling completed for store 106\n",
      "Crawling completed for store 107\n",
      "Crawling completed for store 108\n",
      "Crawling completed for store 109\n",
      "Crawling completed for store 110\n",
      "Crawling completed for store 111\n",
      "Crawling completed for store 112\n",
      "Crawling completed for store 113\n",
      "Crawling completed for store 114\n",
      "Crawling completed for store 115\n",
      "Crawling completed for store 116\n",
      "Crawling completed for store 117\n",
      "Crawling completed for store 118\n",
      "Crawling completed for store 119\n",
      "Crawling completed for store 120\n",
      "Crawling completed for store 121\n",
      "Crawling completed for store 122\n",
      "Crawling completed for store 123\n",
      "Crawling completed for store 124\n",
      "Crawling completed for store 125\n",
      "Crawling completed for store 126\n",
      "Crawling completed for store 127\n",
      "Crawling completed for store 128\n",
      "Crawling completed for store 129\n",
      "Crawling completed for store 130\n",
      "Crawling completed for store 131\n",
      "Crawling completed for store 132\n",
      "Crawling completed for store 133\n",
      "Crawling completed for store 134\n",
      "Crawling completed for store 135\n",
      "Crawling completed for store 136\n",
      "Crawling completed for store 137\n",
      "Crawling completed for store 138\n",
      "Crawling completed for store 139\n",
      "Crawling completed for store 140\n",
      "Crawling completed for store 141\n",
      "Crawling completed for store 142\n",
      "Crawling completed for store 143\n",
      "Crawling completed for store 144\n",
      "Crawling completed for store 145\n",
      "Crawling completed for store 146\n",
      "Crawling completed for store 147\n",
      "Crawling completed for store 148\n",
      "Crawling completed for store 149\n",
      "Crawling completed for store 150\n",
      "Crawling completed for store 151\n",
      "Crawling completed for store 152\n",
      "Crawling completed for store 153\n",
      "Crawling completed for store 154\n",
      "Crawling completed for store 155\n",
      "Crawling completed for store 156\n",
      "Crawling completed for store 157\n",
      "Crawling completed for store 158\n",
      "Crawling completed for store 159\n",
      "Crawling completed for store 160\n",
      "Crawling completed for store 161\n",
      "Crawling completed for store 162\n",
      "Crawling completed for store 163\n",
      "Crawling completed for store 164\n",
      "Crawling completed for store 165\n",
      "Crawling completed for store 166\n",
      "Crawling completed for store 167\n",
      "Crawling completed for store 168\n",
      "Crawling completed for store 169\n",
      "Crawling completed for store 170\n",
      "Crawling completed for store 171\n",
      "Crawling completed for store 172\n",
      "Crawling completed for store 173\n",
      "Crawling completed for store 174\n",
      "Crawling completed for store 175\n",
      "Crawling completed for store 176\n",
      "Crawling completed for store 177\n",
      "Crawling completed for store 178\n",
      "Crawling completed for store 179\n",
      "Crawling completed for store 180\n",
      "Crawling completed for store 181\n",
      "Crawling completed for store 182\n",
      "Crawling completed for store 183\n",
      "Crawling completed for store 184\n",
      "Crawling completed for store 185\n",
      "Crawling completed for store 186\n",
      "Crawling completed for store 187\n",
      "Crawling completed for store 188\n",
      "Crawling completed for store 189\n",
      "Crawling completed for store 190\n",
      "Crawling completed for store 191\n",
      "Crawling completed for store 192\n",
      "Crawling completed for store 193\n",
      "Crawling completed for store 194\n",
      "Crawling completed for store 195\n",
      "Crawling completed for store 196\n",
      "Crawling completed for store 197\n",
      "Crawling completed for store 198\n",
      "Crawling completed for store 199\n",
      "Crawling completed for store 200\n",
      "Crawling completed for store 201\n",
      "Crawling completed for store 202\n",
      "Crawling completed for store 203\n",
      "Crawling completed for store 204\n",
      "Crawling completed for store 205\n",
      "Crawling completed for store 206\n",
      "Crawling completed for store 207\n",
      "Crawling completed for store 208\n",
      "Crawling completed for store 209\n",
      "Crawling completed for store 210\n",
      "Crawling completed for store 211\n",
      "Crawling completed for store 212\n",
      "Crawling completed for store 213\n",
      "Crawling completed for store 214\n",
      "Crawling completed for store 215\n",
      "Crawling completed for store 216\n",
      "Crawling completed for store 217\n",
      "Crawling completed for store 218\n",
      "Crawling completed for store 219\n",
      "Crawling completed for store 220\n",
      "Crawling completed for store 221\n",
      "Crawling completed for store 222\n",
      "Crawling completed for store 223\n",
      "Crawling completed for store 224\n",
      "Crawling completed for store 225\n",
      "Crawling completed for store 226\n",
      "Crawling completed for store 227\n",
      "Crawling completed for store 228\n",
      "Crawling completed for store 229\n",
      "Crawling completed for store 230\n",
      "Crawling completed for store 231\n",
      "Crawling completed for store 232\n",
      "Crawling completed for store 233\n",
      "Crawling completed for store 234\n",
      "Crawling completed for store 235\n",
      "Crawling completed for store 236\n",
      "Crawling completed for store 237\n",
      "Crawling completed for store 238\n",
      "Crawling completed for store 239\n",
      "Crawling completed for store 240\n",
      "Crawling completed for store 241\n",
      "Crawling completed for store 242\n",
      "Crawling completed for store 243\n",
      "Crawling completed for store 244\n",
      "Crawling completed for store 245\n",
      "Crawling completed for store 246\n",
      "Crawling completed for store 247\n",
      "Crawling completed for store 248\n",
      "Crawling completed for store 249\n",
      "Crawling completed for store 250\n",
      "Crawling completed for store 251\n",
      "Crawling completed for store 252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling completed for store 253\n",
      "Crawling completed for store 254\n",
      "Crawling completed for store 255\n",
      "Crawling completed for store 256\n",
      "Crawling completed for store 257\n",
      "Crawling completed for store 258\n",
      "Crawling completed for store 259\n",
      "Crawling completed for store 260\n",
      "Crawling completed for store 261\n",
      "Crawling completed for store 262\n",
      "Crawling completed for store 263\n",
      "Crawling completed for store 264\n",
      "Crawling completed for store 265\n",
      "Crawling completed for store 266\n",
      "Crawling completed for store 267\n",
      "Crawling completed for store 268\n",
      "Crawling completed for store 269\n",
      "Crawling completed for store 270\n",
      "Crawling completed for store 271\n",
      "Crawling completed for store 272\n",
      "Crawling completed for store 273\n",
      "Crawling completed for store 274\n",
      "Crawling completed for store 275\n",
      "Crawling completed for store 276\n",
      "Crawling completed for store 277\n",
      "Crawling completed for store 278\n",
      "Crawling completed for store 279\n",
      "Crawling completed for store 280\n",
      "Crawling completed for store 281\n",
      "Crawling completed for store 282\n",
      "Crawling completed for store 283\n",
      "Crawling completed for store 284\n",
      "Crawling completed for store 285\n",
      "Crawling completed for store 286\n",
      "Crawling completed for store 287\n",
      "Crawling completed for store 288\n",
      "Crawling completed for store 289\n",
      "Crawling completed for store 290\n",
      "Crawling completed for store 291\n",
      "Crawling completed for store 292\n",
      "Crawling completed for store 293\n",
      "Crawling completed for store 294\n",
      "Crawling completed for store 295\n",
      "Crawling completed for store 296\n",
      "Crawling completed for store 297\n",
      "Crawling completed for store 298\n",
      "Crawling completed for store 299\n",
      "Crawling completed for store 300\n",
      "Crawling completed for store 301\n",
      "Crawling completed for store 302\n",
      "Crawling completed for store 303\n",
      "Crawling completed for store 304\n",
      "Crawling completed for store 305\n",
      "Crawling completed for store 306\n",
      "Crawling completed for store 307\n",
      "Crawling completed for store 308\n",
      "Crawling completed for store 309\n",
      "Crawling completed for store 310\n",
      "Crawling completed for store 311\n",
      "Crawling completed for store 312\n",
      "Crawling completed for store 313\n",
      "Crawling completed for store 314\n",
      "Crawling completed for store 315\n",
      "Crawling completed for store 316\n",
      "Crawling completed for store 317\n",
      "Crawling completed for store 318\n",
      "Crawling completed for store 319\n",
      "Crawling completed for store 320\n",
      "Crawling completed for store 321\n",
      "Crawling completed for store 322\n",
      "Crawling completed for store 323\n",
      "Crawling completed for store 324\n",
      "Crawling completed for store 325\n",
      "Crawling completed for store 326\n",
      "Crawling completed for store 327\n",
      "Crawling completed for store 328\n",
      "Crawling completed for store 329\n",
      "Crawling completed for store 330\n",
      "Crawling completed for store 331\n",
      "Crawling completed for store 332\n",
      "Crawling completed for store 333\n",
      "Crawling completed for store 334\n",
      "Crawling completed for store 335\n",
      "Crawling completed for store 336\n",
      "Crawling completed for store 337\n",
      "Crawling completed for store 338\n",
      "Crawling completed for store 339\n",
      "Crawling completed for store 340\n",
      "Crawling completed for store 341\n",
      "Crawling completed for store 342\n",
      "Crawling completed for store 343\n",
      "Crawling completed for store 344\n",
      "Crawling completed for store 345\n",
      "Crawling completed for store 346\n",
      "Crawling completed for store 347\n",
      "Crawling completed for store 348\n",
      "Crawling completed for store 349\n",
      "Crawling completed for store 350\n",
      "Crawling completed for store 351\n",
      "Crawling completed for store 352\n",
      "Crawling completed for store 353\n",
      "Crawling completed for store 354\n",
      "Crawling completed for store 355\n",
      "Crawling completed for store 356\n",
      "Crawling completed for store 357\n",
      "Crawling completed for store 358\n",
      "Crawling completed for store 359\n",
      "Crawling completed for store 360\n",
      "Crawling completed for store 361\n",
      "Crawling completed for store 362\n",
      "Crawling completed for store 363\n",
      "Crawling completed for store 364\n",
      "Crawling completed for store 365\n",
      "Crawling completed for store 366\n",
      "Crawling completed for store 367\n",
      "Crawling completed for store 368\n",
      "Crawling completed for store 369\n",
      "Crawling completed for store 370\n",
      "Crawling completed for store 371\n",
      "Crawling completed for store 372\n",
      "Crawling completed for store 373\n",
      "Crawling completed for store 374\n",
      "Crawling completed for store 375\n",
      "Crawling completed for store 376\n",
      "Crawling completed for store 377\n",
      "Crawling completed for store 378\n",
      "Crawling completed for store 379\n",
      "Crawling completed for store 380\n",
      "Crawling completed for store 381\n",
      "Crawling completed for store 382\n",
      "Crawling completed for store 383\n",
      "Crawling completed for store 384\n",
      "Crawling completed for store 385\n",
      "Crawling completed for store 386\n",
      "Crawling completed for store 387\n",
      "Crawling completed for store 388\n",
      "Crawling completed for store 389\n",
      "Crawling completed for store 390\n",
      "Crawling completed for store 391\n",
      "Crawling completed for store 392\n",
      "Crawling completed for store 393\n",
      "Crawling completed for store 394\n",
      "Crawling completed for store 395\n",
      "Crawling completed for store 396\n",
      "Crawling completed for store 397\n",
      "Crawling completed for store 398\n",
      "Crawling completed for store 399\n",
      "Crawling completed for store 400\n",
      "Crawling completed for store 401\n",
      "Crawling completed for store 402\n",
      "Crawling completed for store 403\n",
      "Crawling completed for store 404\n",
      "Crawling completed for store 405\n",
      "Crawling completed for store 406\n",
      "Crawling completed for store 407\n",
      "Crawling completed for store 408\n",
      "Crawling completed for store 409\n",
      "Crawling completed for store 410\n",
      "Crawling completed for store 411\n",
      "Crawling completed for store 412\n",
      "Crawling completed for store 413\n",
      "Crawling completed for store 414\n",
      "Crawling completed for store 415\n",
      "Crawling completed for store 416\n",
      "Crawling completed for store 417\n",
      "Crawling completed for store 418\n",
      "Crawling completed for store 419\n",
      "Crawling completed for store 420\n",
      "Crawling completed for store 421\n",
      "Crawling completed for store 422\n",
      "Crawling completed for store 423\n",
      "Crawling completed for store 424\n",
      "Crawling completed for store 425\n",
      "Crawling completed for store 426\n",
      "Crawling completed for store 427\n",
      "Crawling completed for store 428\n",
      "Crawling completed for store 429\n",
      "Crawling completed for store 430\n",
      "Crawling completed for store 431\n",
      "Crawling completed for store 432\n",
      "Crawling completed for store 433\n",
      "Crawling completed for store 434\n",
      "Crawling completed for store 435\n",
      "Crawling completed for store 436\n",
      "Crawling completed for store 437\n",
      "Crawling completed for store 438\n",
      "Crawling completed for store 439\n",
      "Crawling completed for store 440\n",
      "Crawling completed for store 441\n",
      "Crawling completed for store 442\n",
      "Crawling completed for store 443\n",
      "Crawling completed for store 444\n",
      "Crawling completed for store 445\n",
      "Crawling completed for store 446\n",
      "Crawling completed for store 447\n",
      "Crawling completed for store 448\n",
      "Crawling completed for store 449\n",
      "Crawling completed for store 450\n",
      "Crawling completed for store 451\n",
      "Crawling completed for store 452\n",
      "Crawling completed for store 453\n",
      "Crawling completed for store 454\n",
      "Crawling completed for store 455\n",
      "Crawling completed for store 456\n",
      "Crawling completed for store 457\n",
      "Crawling completed for store 458\n",
      "Crawling completed for store 459\n",
      "Crawling completed for store 460\n",
      "Crawling completed for store 461\n",
      "Crawling completed for store 462\n",
      "Crawling completed for store 463\n",
      "Crawling completed for store 464\n",
      "Crawling completed for store 465\n",
      "Crawling completed for store 466\n",
      "Crawling completed for store 467\n",
      "Crawling completed for store 468\n",
      "Crawling completed for store 469\n",
      "Crawling completed for store 470\n",
      "Crawling completed for store 471\n",
      "Crawling completed for store 472\n",
      "Crawling completed for store 473\n",
      "Crawling completed for store 474\n",
      "Crawling completed for store 475\n",
      "Crawling completed for store 476\n",
      "Crawling completed for store 477\n",
      "Crawling completed for store 478\n",
      "Crawling completed for store 479\n",
      "Crawling completed for store 480\n",
      "Crawling completed for store 481\n",
      "Crawling completed for store 482\n",
      "Crawling completed for store 483\n",
      "Crawling completed for store 484\n",
      "Crawling completed for store 485\n",
      "Crawling completed for store 486\n",
      "Crawling completed for store 487\n",
      "Crawling completed for store 488\n",
      "Crawling completed for store 489\n",
      "Crawling completed for store 490\n",
      "Crawling completed for store 491\n",
      "Crawling completed for store 492\n",
      "Crawling completed for store 493\n",
      "Crawling completed for store 494\n",
      "Crawling completed for store 495\n",
      "Crawling completed for store 496\n",
      "Crawling completed for store 497\n",
      "Crawling completed for store 498\n",
      "Crawling completed for store 499\n",
      "Crawling completed for store 500\n",
      "Crawling completed for store 501\n",
      "Crawling completed for store 502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling completed for store 503\n",
      "Crawling completed for store 504\n",
      "Crawling completed for store 505\n",
      "Crawling completed for store 506\n",
      "Crawling completed for store 507\n",
      "Crawling completed for store 508\n",
      "Crawling completed for store 509\n",
      "Crawling completed for store 510\n",
      "Crawling completed for store 511\n",
      "Crawling completed for store 512\n",
      "Crawling completed for store 513\n",
      "Crawling completed for store 514\n",
      "Crawling completed for store 515\n",
      "Crawling completed for store 516\n",
      "Crawling completed for store 517\n",
      "Crawling completed for store 518\n",
      "Crawling completed for store 519\n",
      "Crawling completed for store 520\n",
      "Crawling completed for store 521\n",
      "Crawling completed for store 522\n",
      "Crawling completed for store 523\n",
      "Crawling completed for store 524\n",
      "Crawling completed for store 525\n",
      "Crawling completed for store 526\n",
      "Crawling completed for store 527\n",
      "Crawling completed for store 528\n",
      "Crawling completed for store 529\n",
      "Crawling completed for store 530\n",
      "Crawling completed for store 531\n",
      "Crawling completed for store 532\n",
      "Crawling completed for store 533\n",
      "Crawling completed for store 534\n",
      "Crawling completed for store 535\n",
      "Crawling completed for store 536\n",
      "Crawling completed for store 537\n",
      "Crawling completed for store 538\n",
      "Crawling completed for store 539\n"
     ]
    }
   ],
   "source": [
    "# '가게 정보' 크롤링\n",
    "# 파일 로드\n",
    "result1 = pd.read_csv('가게소개용.csv', encoding='cp949')\n",
    "\n",
    "# 웹드라이버 옵션 설정\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "\n",
    "# 웹드라이버 초기화\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "\n",
    "description = []\n",
    "store_type = []  # List to store store type text\n",
    "review_counts = []  # List to store review counts\n",
    "sub_counter = 0\n",
    "counter = 0\n",
    "sub_sub_counter = 0\n",
    "\n",
    "# URL에 접속하여 크롤링 수행\n",
    "for idx in range(len(result1['가게정보'])):\n",
    "    driver.get(result1['가게정보'][idx])\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "\n",
    "    # 더보기 버튼이 있다면 클릭\n",
    "    try:\n",
    "        more_button = driver.find_element_by_css_selector('a.xHaT3 span.rvCSr')\n",
    "        more_button.click()\n",
    "        # 대기\n",
    "        WebDriverWait(driver, 3)\n",
    "    except Exception:\n",
    "        pass  # 더보기 버튼이 없는 경우, 그냥 넘어감\n",
    "\n",
    "    # 페이지 로딩이 완료된 후 HTML 소스 가져오기\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    # span 태그 내의 텍스트를 추출 (Description)\n",
    "    try:\n",
    "        desc = soup.find('a', {'class': 'xHaT3'}).find('span', {'class': 'zPfVt'}).text\n",
    "        description.append(desc)\n",
    "    except AttributeError:\n",
    "        description.append(\"Description not found\")\n",
    "\n",
    "    # Extracting store type\n",
    "    try:\n",
    "        store = soup.find('span', {'class': 'DJJvD'}).text\n",
    "        store_type.append(store)\n",
    "    except AttributeError:\n",
    "        store_type.append(\"Store type not found\")\n",
    "\n",
    "    # Extracting review counts\n",
    "    try:\n",
    "        review_count = soup.find('div', {'class': 'dAsGb'}).find('a', role=\"button\").find('em').text\n",
    "        review_counts.append(review_count)\n",
    "    except AttributeError:\n",
    "        review_counts.append(\"Review counts not found\")\n",
    "    \n",
    "    print(f\"Crawling completed for store {idx + 1}\")\n",
    "\n",
    "# 데이터프레임에 설명, 상점 유형, 리뷰 개수 컬럼 추가\n",
    "result1['Description'] = description\n",
    "result1['Store_Type'] = store_type\n",
    "result1['Review_Counts'] = review_counts  # Adding review counts to DataFrame\n",
    "\n",
    "# 데이터프레임으로 저장\n",
    "result1.to_csv('가게정보크롤링.csv', encoding='utf-8-sig', index=False)\n",
    "\n",
    "# 웹드라이버 종료\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
